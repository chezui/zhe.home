{
  "hash": "b67bc5d8fb3611b2369539bc3762a8e3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"[WIP] Selection Bias in Recommender Systems\"\nauthor:\n  name: \"Zhe Cui\"\ndate: 2025-07-07\nformat:\n  html:\n    toc: true\n    code-fold: true\n    code-echo: false\n    code-line-numbers: true\n    number-sections: false\n    html-math-method: katex\n    fig_retina: 2\n    fig_width: 6\n    fig_height: 4\n    dpi: 400\ndraft: true\nexecute:\n  freeze: auto\nabstract:\n  Selection Bias in Recommender Systems\n---\n\n## Problem Statement\n\nThe setup: we have a recommender system that serves items to users that we believe they will find engaging, we then log and analyze the engagement data to find insights for improvements. We find that videos about red pandas get 10x more watch time and 5x more shares than videos about model airplanes.\n\nWhen we use engagement logs from a recommender systems, we need to address the issue of selection bias. Using the engagement log data naively to make design and business decisions is not a good idea due to two major sources of recommendation bias:\n\n  1. **Selection Bias**: Users only engage with items that we recommend them that we serve based on our ranking models expecting these items will be engaging (watch time, clicks, shares etc.).\n  2. **Positional Bias**: Users are more likely to engage with items that are placed higher in the feed (e.g. first few items in the feed).\n\n```{mermaid, [^Flow]}\ngraph LR\n    user[User] --> recommender[Recommender]\n    recommender --> exposure[Exposure]\n    exposure --> action[Action]\n    action --> log[Log]\n```\n\nTODO make the flow diagram clearer... something doesn't seem right.\n\n```{sql}\nSELECT \n  user_id,\n  item_id,\n  event_time,\n  watch_time,\n  num_likes,\n  num_shares,\n  num_subscribes,\n  num_hides,\n  num_reports\nFROM engagement_log\n```\n\n\n\n\n## Quarto Features\n\nQuarto supports a variety of features including citations, cross-references, and more. Here's an example of a citation:\n\n> \"Quarto is a next-generation open-source scientific and technical publishing system.\" [@quarto]\n\n## References\n\n[^quarto]: Quarto is a next-generation open-source scientific and technical publishing system.  \n\n## Ideas\n\nPerfect — a blog post forces clarity *and* accessibility. You want it to feel like:\n👉 *“Here’s the real problem. Here’s why naive approaches fail. Here’s how smart people tackle it (and the limits).”*\n\nHere’s a crisp **outline** I’d recommend for your blog post **“Selection Bias in Recommender System Engagement Logs”** — designed to teach *your reader* and *yourself*:\n\n---\n\n### 📌 **Title:**\n\n**Selection Bias in Recommender System Engagement Logs: The Hidden Trap in Offline Evaluation (and How to Fight It)**\n\n---\n\n### 🔹 **Intro (set the hook)**\n\n* Start with a real-world example: *“Imagine you’re testing a new recommender model offline. The logs say item A has double the click rate of item B — so you boost A. But when you launch, engagement drops. What happened?”*\n* State the thesis: *Offline metrics can lie because of selection bias. This post explains why, and what to do about it.*\n\n---\n\n### 🔹 **The core problem: Why your engagement logs are biased**\n\n* Diagram: *user → recommender → exposure → action → log*\n* Explain exposure bias, positional bias, user bias.\n* Show (or describe) a simple example: *e.g. top slot always gets 10x clicks, no matter what you put there.*\n\n---\n\n### 🔹 **Where this hurts us**\n\n* Offline evaluation: why naive click rates overestimate or underestimate true performance\n* Training models: why we learn the wrong preferences\n* Marketplace fairness: why some items or creators stay buried\n\n---\n\n### 🔹 **The statistical fix: Inverse Propensity Score Weighting**\n\n* Intuition: *“Adjust for how likely an item was to be shown in the first place.”*\n* Show formula: unbiased estimator of engagement\n* Example: walk through toy data (small table: item, propensity, click, IPS weight)\n* Discuss variance: why small propensities are dangerous\n* Mitigation: clipping, self-normalized IPS\n\n---\n\n### 🔹 **Beyond IPS**\n\n* Doubly robust estimators\n* Direct modeling / causal models\n* Design for exploration (logging randomization)\n\n---\n\n### 🔹 **Practical guidance**\n\n* If you don’t know the logging propensities, what can you do?\n* When is IPS worth using, when is it overkill?\n* How to design your system to reduce selection bias at the source\n\n---\n\n### 🔹 **Conclusion**\n\n* Reinforce takeaway: *“Logs don’t tell you the full truth. Selection bias is the silent killer of offline evaluation — but with the right tools, you can fight it.”*\n\n---\n\n### ✅ **Why this will deepen your learning**\n\nYou’ll have to:\n\n* Articulate the problem clearly\n* Do the math and make it accessible\n* Code an example (even if just for yourself)\n* Think critically about trade-offs\n\n---\n\nIf you like, I can help you:\n👉 draft the intro hook\n👉 sketch the toy example data + math\n👉 help you build a Quarto / Jupyter notebook to embed in your blog\n\nAre you thinking of publishing on Medium, personal site, or somewhere else?\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}