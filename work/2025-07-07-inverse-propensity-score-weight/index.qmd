---
title: "[WIP] Selection Bias in Recommender Systems"
author:
  name: "Zhe Cui"
date: 2025-07-07
format:
  html:
    toc: true
    code-fold: true
    code-echo: false
    code-line-numbers: true
    number-sections: false
    html-math-method: katex
    fig_retina: 2
    fig_width: 6
    fig_height: 4
    dpi: 400
draft: true 
execute:
  freeze: auto
abstract:
  Selection Bias in Recommender Systems
---

## Problem Statement

The setup: we have a recommender system that serves items to users that we believe they will find engaging, we then log and analyze the engagement data to find insights for improvements. We find that videos about red pandas get 10x more watch time and 5x more shares than videos about model airplanes.

When we use engagement logs from a recommender systems, we need to address the issue of selection bias. Using the engagement log data naively to make design and business decisions is not a good idea due to two major sources of recommendation bias:

  1. **Selection Bias**: Users only engage with items that we recommend them that we serve based on our ranking models expecting these items will be engaging (watch time, clicks, shares etc.).
  2. **Positional Bias**: Users are more likely to engage with items that are placed higher in the feed (e.g. first few items in the feed).

```{mermaid, [^Flow]}
graph LR
    user[User] --> recommender[Recommender]
    recommender --> exposure[Exposure]
    exposure --> action[Action]
    action --> log[Log]
```






## Quarto Features

Quarto supports a variety of features including citations, cross-references, and more. Here's an example of a citation:

> "Quarto is a next-generation open-source scientific and technical publishing system." [@quarto]

## References

[^quarto]: Quarto is a next-generation open-source scientific and technical publishing system.  

## Ideas

Perfect â€” a blog post forces clarity *and* accessibility. You want it to feel like:
ğŸ‘‰ *â€œHereâ€™s the real problem. Hereâ€™s why naive approaches fail. Hereâ€™s how smart people tackle it (and the limits).â€*

Hereâ€™s a crisp **outline** Iâ€™d recommend for your blog post **â€œSelection Bias in Recommender System Engagement Logsâ€** â€” designed to teach *your reader* and *yourself*:

---

### ğŸ“Œ **Title:**

**Selection Bias in Recommender System Engagement Logs: The Hidden Trap in Offline Evaluation (and How to Fight It)**

---

### ğŸ”¹ **Intro (set the hook)**

* Start with a real-world example: *â€œImagine youâ€™re testing a new recommender model offline. The logs say item A has double the click rate of item B â€” so you boost A. But when you launch, engagement drops. What happened?â€*
* State the thesis: *Offline metrics can lie because of selection bias. This post explains why, and what to do about it.*

---

### ğŸ”¹ **The core problem: Why your engagement logs are biased**

* Diagram: *user â†’ recommender â†’ exposure â†’ action â†’ log*
* Explain exposure bias, positional bias, user bias.
* Show (or describe) a simple example: *e.g. top slot always gets 10x clicks, no matter what you put there.*

---

### ğŸ”¹ **Where this hurts us**

* Offline evaluation: why naive click rates overestimate or underestimate true performance
* Training models: why we learn the wrong preferences
* Marketplace fairness: why some items or creators stay buried

---

### ğŸ”¹ **The statistical fix: Inverse Propensity Score Weighting**

* Intuition: *â€œAdjust for how likely an item was to be shown in the first place.â€*
* Show formula: unbiased estimator of engagement
* Example: walk through toy data (small table: item, propensity, click, IPS weight)
* Discuss variance: why small propensities are dangerous
* Mitigation: clipping, self-normalized IPS

---

### ğŸ”¹ **Beyond IPS**

* Doubly robust estimators
* Direct modeling / causal models
* Design for exploration (logging randomization)

---

### ğŸ”¹ **Practical guidance**

* If you donâ€™t know the logging propensities, what can you do?
* When is IPS worth using, when is it overkill?
* How to design your system to reduce selection bias at the source

---

### ğŸ”¹ **Conclusion**

* Reinforce takeaway: *â€œLogs donâ€™t tell you the full truth. Selection bias is the silent killer of offline evaluation â€” but with the right tools, you can fight it.â€*

---

### âœ… **Why this will deepen your learning**

Youâ€™ll have to:

* Articulate the problem clearly
* Do the math and make it accessible
* Code an example (even if just for yourself)
* Think critically about trade-offs

---

If you like, I can help you:
ğŸ‘‰ draft the intro hook
ğŸ‘‰ sketch the toy example data + math
ğŸ‘‰ help you build a Quarto / Jupyter notebook to embed in your blog

Are you thinking of publishing on Medium, personal site, or somewhere else?
